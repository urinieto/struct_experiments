{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.stats.multicomp import (pairwise_tukeyhsd,\n",
    "                                         MultiComparison)\n",
    "\n",
    "import msaf\n",
    "\n",
    "# Plotting settings\n",
    "%matplotlib inline\n",
    "sns.set_style(\"dark\")\n",
    "\n",
    "ds_base = \"/home/uri/datasets/\"\n",
    "dataset_path = \"/home/uri/datasets/BeatlesTUT/\"\n",
    "# dataset_path = \"/Users/uriadmin/datasets/BeatlesTUT/\"\n",
    "n_jobs = 8\n",
    "n_octaves = [4, 5, 6, 7]\n",
    "f_mins = [27.5 * 2 ** (i / 12.) for i in xrange(0, 12, 2)]\n",
    "labels_ids = [\"scluster\", \"siplca\", \"fmc2d\", \"cnmf\", \"cc\"]\n",
    "bounds_ids = [\"sf\", \"cnmf\", \"foote\", \"cc\", \"olda\", \"scluster\", \"siplca\"]\n",
    "features = [\"hpcp\", \"mfcc\", \"tonnetz\", \"cqt\"]\n",
    "n_mffc_coeffs = range(7, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Boundaries for MFCC\n",
    "feature = \"mfcc\"\n",
    "for n_mfcc_coeff in n_mffc_coeffs:\n",
    "    print \"MFCC Coeffs: \", n_mfcc_coeff\n",
    "    msaf.Anal.mfcc_coeff = n_mfcc_coeff\n",
    "    msaf.featextract.process(dataset_path, n_jobs=n_jobs, overwrite=True)\n",
    "    for bound_id in bounds_ids:\n",
    "        print \"\\t bounds_id:\", bound_id\n",
    "        results = msaf.process(dataset_path, feature=feature, boundaries_id=bound_id, n_jobs=n_jobs)\n",
    "        results = msaf.eval.process(dataset_path, bound_id, feature=feature, save=True, n_jobs=n_jobs)\n",
    "    key = \"mfcc_coeffE%d\" % n_mfcc_coeff\n",
    "    shutil.move(os.path.join(dataset_path, \"features\"), os.path.join(dataset_path, \"features_%s\" % key))\n",
    "    shutil.move(\"results\", \"results_%s\" % key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC Coeffs:  7\n",
      "\t bounds_id: fmc2d\n",
      "\t bounds_id: cnmf\n",
      "\t bounds_id: cc\n",
      "MFCC Coeffs:  8\n",
      "\t bounds_id: fmc2d\n",
      "\t bounds_id: cnmf\n",
      "\t bounds_id: cc\n",
      "MFCC Coeffs:  9\n",
      "\t bounds_id: fmc2d\n",
      "\t bounds_id: cnmf\n",
      "\t bounds_id: cc\n",
      "MFCC Coeffs:  10\n",
      "\t bounds_id: fmc2d\n",
      "\t bounds_id: cnmf\n",
      "\t bounds_id: cc\n",
      "MFCC Coeffs:  11\n",
      "\t bounds_id: fmc2d\n",
      "\t bounds_id: cnmf\n",
      "\t bounds_id: cc\n",
      "MFCC Coeffs:  12\n",
      "\t bounds_id: fmc2d\n",
      "\t bounds_id: cnmf\n",
      "\t bounds_id: cc\n",
      "MFCC Coeffs:  13\n",
      "\t bounds_id: fmc2d\n",
      "\t bounds_id: cnmf\n",
      "\t bounds_id: cc\n",
      "MFCC Coeffs:  14\n",
      "\t bounds_id: fmc2d\n",
      "\t bounds_id: cnmf\n",
      "\t bounds_id: cc\n",
      "MFCC Coeffs:  15\n",
      "\t bounds_id: fmc2d\n",
      "\t bounds_id: cnmf\n",
      "\t bounds_id: cc\n",
      "MFCC Coeffs:  16\n",
      "\t bounds_id: fmc2d\n",
      "\t bounds_id: cnmf\n",
      "\t bounds_id: cc\n",
      "MFCC Coeffs:  17\n",
      "\t bounds_id: fmc2d\n",
      "\t bounds_id: cnmf\n",
      "\t bounds_id: cc\n",
      "MFCC Coeffs:  18\n",
      "\t bounds_id: fmc2d\n",
      "\t bounds_id: cnmf\n",
      "\t bounds_id: cc\n",
      "MFCC Coeffs:  19\n",
      "\t bounds_id: fmc2d\n",
      "\t bounds_id: cnmf\n",
      "\t bounds_id: cc\n"
     ]
    }
   ],
   "source": [
    "# Labels for MFCC (Assuming features have already been computed)\n",
    "for n_mfcc_coeff in n_mffc_coeffs:\n",
    "    print \"MFCC Coeffs: \", n_mfcc_coeff\n",
    "    msaf.Anal.mfcc_coeff = n_mfcc_coeff\n",
    "    key = \"mfcc_coeffE%d\" % n_mfcc_coeff\n",
    "    shutil.move(os.path.join(dataset_path, \"features_%s\" % key), os.path.join(dataset_path, \"features\"))\n",
    "    for label_id in labels_ids:\n",
    "        print \"\\t bounds_id:\", label_id\n",
    "        results = msaf.process(dataset_path, feature=feature, labels_id=label_id, n_jobs=n_jobs)\n",
    "        results = msaf.eval.process(dataset_path, \"gt\", labels_id=label_id, feature=feature, save=True, n_jobs=n_jobs)\n",
    "    shutil.move(os.path.join(dataset_path, \"features\"), os.path.join(dataset_path, \"features_%s\" % key))\n",
    "    shutil.move(\"results\", \"results_%s\" % key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Boundaries\n",
    "for n_octave in n_octaves:\n",
    "    for f_min in f_mins:\n",
    "        msaf.Anal.f_min = f_min\n",
    "        msaf.Anal.n_octaves = n_octave\n",
    "        msaf.featextract.process(dataset_path, n_jobs=n_jobs, overwrite=True)\n",
    "        for bound_id in bounds_ids:\n",
    "            results = msaf.process(dataset_path, feature=\"hpcp\", boundaries_id=bound_id, n_jobs=n_jobs)\n",
    "            results = msaf.eval.process(dataset_path, bound_id, save=True, n_jobs=n_jobs)\n",
    "        key = \"noctavesE%d_fminE%.1f\" % (n_octave, f_min)\n",
    "        shutil.move(os.path.join(dataset_path, \"features\"), os.path.join(dataset_path, \"features_%s\" % key))\n",
    "        shutil.move(\"results\", \"results_%s\" % key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Labels (Assuming features have already been computed)\n",
    "for n_octave in n_octaves:\n",
    "    for f_min in f_mins:\n",
    "        if n_octave == 7 and f_min > 40:\n",
    "            continue\n",
    "        msaf.Anal.f_min = f_min\n",
    "        msaf.Anal.n_octaves = n_octave\n",
    "        key = \"noctavesE%d_fminE%.1f\" % (n_octave, f_min)\n",
    "        shutil.move(os.path.join(dataset_path, \"features_%s\" % key), os.path.join(dataset_path, \"features\"))\n",
    "        for label_id in labels_ids:\n",
    "            results = msaf.process(dataset_path, feature=\"hpcp\", labels_id=label_id, n_jobs=n_jobs)\n",
    "            results = msaf.eval.process(dataset_path, \"gt\", labels_id=label_id, save=True, n_jobs=n_jobs)\n",
    "        shutil.move(os.path.join(dataset_path, \"features\"), os.path.join(dataset_path, \"features_%s\" % key))\n",
    "        shutil.move(\"results\", \"results_%s\" % key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounds cc\n",
      "bounds olda\n",
      "bounds scluster\n",
      "bounds siplca\n"
     ]
    }
   ],
   "source": [
    "# Explore different features\n",
    "# Run boundaries with different features\n",
    "for bounds_id in bounds_ids:\n",
    "    print \"bounds\", bounds_id\n",
    "    if bounds_id != \"siplca\":\n",
    "        continue# Labels (Assuming features have already been computed)\n",
    "for n_octave in n_octaves:\n",
    "    for f_min in f_mins:\n",
    "        if n_octave == 7 and f_min > 40:\n",
    "            continue\n",
    "        msaf.Anal.f_min = f_min\n",
    "        msaf.Anal.n_octaves = n_octave\n",
    "        key = \"noctavesE%d_fminE%.1f\" % (n_octave, f_min)\n",
    "        shutil.move(os.path.join(dataset_path, \"features_%s\" % key), os.path.join(dataset_path, \"features\"))\n",
    "        for label_id in labels_ids:\n",
    "            results = msaf.process(dataset_path, feature=\"hpcp\", labels_id=label_id, n_jobs=n_jobs)\n",
    "            results = msaf.eval.process(dataset_path, \"gt\", labels_id=label_id, save=True, n_jobs=n_jobs)\n",
    "        shutil.move(os.path.join(dataset_path, \"features\"), os.path.join(dataset_path, \"features_%s\" % key))\n",
    "        shutil.move(\"results\", \"results_%s\" % key)\n",
    "    for feature in features:\n",
    "        results = msaf.process(dataset_path, feature=feature, boundaries_id=bounds_id, n_jobs=n_jobs)\n",
    "        results = msaf.eval.process(dataset_path, bounds_id, save=True, n_jobs=n_jobs, feature=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run labels with different features\n",
    "for labels_id in labels_ids:\n",
    "    for feature in features:\n",
    "        try:\n",
    "            results = msaf.process(dataset_path, feature=feature, labels_id=labels_id, n_jobs=n_jobs)\n",
    "            results = msaf.eval.process(dataset_path, \"gt\", labels_id=labels_id, save=True, n_jobs=n_jobs, feature=feature)\n",
    "        except RuntimeError as e:\n",
    "            print \"Warning: \", e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets ##\n",
    "\n",
    "* Celurean\n",
    "* Epiphyte\n",
    "* Isophonics\n",
    "* SALAMI\n",
    "* Beatles\n",
    "* SPAM\n",
    "* Sargon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Explore different datasets\n",
    "dataset_names = [\"Cerulean\", \"Epiphyte\", \"Isophonics\", \"SALAMI\"]\n",
    "feature = \"hpcp\"\n",
    "dataset_path = \"/home/uri/datasets/Segments/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run boundaries with different datasets\n",
    "for ds_name in dataset_names:\n",
    "    print \"Computing boundaries for %s\" % ds_name\n",
    "    for bounds_id in bounds_ids:\n",
    "        results = msaf.process(dataset_path, feature=feature, boundaries_id=bounds_id, \n",
    "                               n_jobs=n_jobs, ds_name=ds_name)\n",
    "        results = msaf.eval.process(dataset_path, bounds_id, save=True, n_jobs=n_jobs, \n",
    "                                    feature=feature, ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run labels with different datasets\n",
    "for ds_name in dataset_names:\n",
    "    print \"Computing labels for %s\" % ds_name\n",
    "    for labels_id in labels_ids:\n",
    "        try:\n",
    "            results = msaf.process(dataset_path, feature=feature, labels_id=labels_id, \n",
    "                                   n_jobs=n_jobs, ds_name=ds_name)\n",
    "            results = msaf.eval.process(dataset_path, \"gt\", labels_id=labels_id, save=True, n_jobs=n_jobs, \n",
    "                                        feature=feature, ds_name=ds_name)\n",
    "        except RuntimeError as e:\n",
    "            print \"Warning: \", e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing boundaries for Beatles-TUT\n",
      "Computing labels for Beatles-TUT\n"
     ]
    }
   ],
   "source": [
    "# Do the same for the Beatles-TUT / SPAM dataset\n",
    "dataset_path = \"/home/uri/datasets/BeatlesTUT/\"\n",
    "dataset_path = \"/home/uri/datasets/Sargon/\"\n",
    "\n",
    "# Run boundaries for the Beatles-TUT\n",
    "print \"Computing boundaries for Beatles-TUT\"\n",
    "for bounds_id in bounds_ids:\n",
    "    results = msaf.process(dataset_path, feature=feature, boundaries_id=bounds_id, \n",
    "                           n_jobs=n_jobs)\n",
    "    results = msaf.eval.process(dataset_path, bounds_id, save=True, n_jobs=n_jobs, \n",
    "                                feature=feature)\n",
    "\n",
    "# Run labels for the Beatles-TUT\n",
    "print \"Computing labels for Beatles-TUT\"\n",
    "for labels_id in labels_ids:\n",
    "    try:\n",
    "        results = msaf.process(dataset_path, feature=feature, labels_id=labels_id, \n",
    "                               n_jobs=n_jobs)\n",
    "        results = msaf.eval.process(dataset_path, \"gt\", labels_id=labels_id, save=True, n_jobs=n_jobs, \n",
    "                                    feature=feature)\n",
    "    except RuntimeError as e:\n",
    "        print \"Warning: \", e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotations ##\n",
    "\n",
    "Cerulean: 6 different annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Explore different datasets\n",
    "ds_name = \"*\"\n",
    "feature = \"hpcp\"\n",
    "dataset_path = \"/home/uri/datasets/SPAM/\"\n",
    "annotators = np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounds cc Annotator 0\n",
      "Bounds cc Annotator 1\n",
      "Bounds cc Annotator 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Couldn't compute the Information Gain for file /home/uri/datasets/SPAM/estimations/SPAM_SALAMI_78.jams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounds cc Annotator 3\n",
      "Bounds cc Annotator 4\n",
      "Computing labels for multiple annotators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Couldn't compute the Information Gain for file /home/uri/datasets/SPAM/estimations/SPAM_Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\n",
      "WARNING:root:Couldn't compute the Information Gain for file /home/uri/datasets/SPAM/estimations/SPAM_SALAMI_78.jams\n",
      "WARNING:root:Couldn't compute the Information Gain for file /home/uri/datasets/SPAM/estimations/SPAM_Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\n"
     ]
    }
   ],
   "source": [
    "# Run boundaries for all the annotators in Cerulean\n",
    "for annotator_id in annotators:\n",
    "    for bounds_id in bounds_ids:\n",
    "        if bounds_id != \"cc\":\n",
    "            continue\n",
    "        print \"Bounds\", bounds_id, \"Annotator\", annotator_id\n",
    "        results = msaf.process(dataset_path, feature=feature, boundaries_id=bounds_id, \n",
    "                               n_jobs=n_jobs, annotator_id=annotator_id, ds_name=ds_name)\n",
    "        results = msaf.eval.process(dataset_path, bounds_id, save=True, n_jobs=n_jobs, \n",
    "                                    feature=feature, annotator_id=annotator_id, ds_name=ds_name)\n",
    "\n",
    "# Run labels for all the annotators in SubSegments\n",
    "print \"Computing labels for multiple annotators\"\n",
    "for annotator_id in annotators:\n",
    "    for labels_id in labels_ids:\n",
    "        if labels_id != \"cc\":\n",
    "            continue\n",
    "        try:\n",
    "            results = msaf.process(dataset_path, feature=feature, labels_id=labels_id, \n",
    "                                   n_jobs=n_jobs, annotator_id=annotator_id, ds_name=ds_name)\n",
    "            results = msaf.eval.process(dataset_path, \"gt\", labels_id=labels_id, save=True, n_jobs=n_jobs, \n",
    "                                        feature=feature, annotator_id=annotator_id, ds_name=ds_name)\n",
    "        except RuntimeError as e:\n",
    "            print \"Warning: \", e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Algorithms #\n",
    "\n",
    "Run all label algorithms with all the different boundaries algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sf', 'cnmf', 'foote', 'cc', 'olda', 'scluster', 'siplca', 'gt']\n"
     ]
    }
   ],
   "source": [
    "# Explore different datasets\n",
    "feature = \"hpcp\"\n",
    "dataset_path = \"/home/uri/datasets/BeatlesTUT/\"\n",
    "bounds_ids += [\"gt\"]\n",
    "annotator_id = 0\n",
    "ds_name=\"*\"\n",
    "print bounds_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing:  sf scluster\n",
      "Computing:  sf siplca\n",
      "Computing:  sf fmc2d\n",
      "Computing:  sf cnmf\n",
      "Computing:  sf cc\n",
      "Computing:  cnmf scluster\n",
      "Computing:  cnmf siplca\n",
      "Computing:  cnmf fmc2d\n",
      "Computing:  cnmf cnmf\n",
      "Computing:  cnmf cc\n",
      "Computing:  foote scluster\n",
      "Computing:  foote siplca\n",
      "Computing:  foote fmc2d\n",
      "Computing:  foote cnmf\n",
      "Computing:  foote cc\n",
      "Computing:  cc scluster\n",
      "Computing:  cc siplca\n",
      "Computing:  cc fmc2d\n",
      "Computing:  cc cnmf\n",
      "Computing:  cc cc\n",
      "Computing:  olda scluster\n",
      "Computing:  olda siplca\n",
      "Computing:  olda fmc2d\n",
      "Computing:  olda cnmf\n",
      "Computing:  olda cc\n",
      "Computing:  scluster scluster\n",
      "Computing:  scluster siplca\n",
      "Computing:  scluster fmc2d\n",
      "Computing:  scluster cnmf\n",
      "Computing:  scluster cc\n",
      "Computing:  siplca scluster\n",
      "Computing:  siplca siplca\n",
      "Computing:  siplca fmc2d\n",
      "Computing:  siplca cnmf\n",
      "Computing:  siplca cc\n",
      "Computing:  gt scluster\n",
      "Computing:  gt siplca\n",
      "Computing:  gt fmc2d\n",
      "Computing:  gt cnmf\n",
      "Computing:  gt cc\n"
     ]
    }
   ],
   "source": [
    "for bounds_id in bounds_ids:\n",
    "    for labels_id in labels_ids:\n",
    "            print \"Computing: \", bounds_id, labels_id\n",
    "            results = msaf.process(dataset_path, feature=feature, boundaries_id=bounds_id, labels_id=labels_id, \n",
    "                                   n_jobs=n_jobs, annotator_id=annotator_id, ds_name=ds_name)\n",
    "            results = msaf.eval.process(dataset_path, bounds_id, labels_id=labels_id, save=True, n_jobs=n_jobs, \n",
    "                                        feature=feature, annotator_id=annotator_id, ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
